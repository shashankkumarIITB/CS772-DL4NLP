# -*- coding: utf-8 -*-
"""NLP4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WxiZcT-i1LSYvy8sUeHvM2t1-bC3Xz1N
"""

import sklearn
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow import keras
from tensorflow.keras import Model
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Flatten, Dense, Activation, Embedding, InputLayer, Dropout, BatchNormalization
from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Dropout, Bidirectional
from tensorflow.keras.models import load_model
from tensorflow.keras.metrics import Precision, Recall
from tensorflow.keras.initializers import Constant
from tensorflow.keras.callbacks import EarlyStopping

import os,csv,re, random
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

import nltk
from nltk.corpus import stopwords
import gensim

nltk.download('stopwords')
# import gensim.downloader as api
# path = api.load('glove-wiki-gigaword-300', return_path=True)
# print(path)
# !rm glove*
# !wget -P '/content/drive/MyDrive/NLP4/' http://nlp.stanford.edu/data/glove.840B.300d.zip
# !unzip '/content/drive/MyDrive/NLP4/glove*.zip' -d '/content/drive/MyDrive/NLP4/'

# from google.colab import drive
# drive.mount('/content/drive')

def loadGloveModel(File):
  print("Loading Glove Model")
  f = open(File,'r')
  gloveModel = {}
  for line in f:
    try:
      splitLines = line.split()
      word = splitLines[0]
      wordEmbedding = np.array([float(value) for value in splitLines[1:]])
      gloveModel[word] = wordEmbedding
    except ValueError:
      print(splitLines,len(splitLines))
      print(len(gloveModel)," words loaded!")
  return gloveModel

# glove=loadGloveModel('/content/drive/MyDrive/embeddings/glove.840B.300d.txt')
glove=loadGloveModel('embeddings/glove.6B.300d.txt')

# Maxlength of the sequence of input words
MAX_INPUT_LENGTH = 30
# Maximum output ratings
MAX_RATINGS = 5
# Dictionary and index for encoding
UNKNOWN_TOKEN = 'UKN'
WORD_TO_INDEX = {UNKNOWN_TOKEN: 0}
# Embeddings to be used
EMBEDDING_DIM = 300
EMBEDDING_MATRIX = np.array([np.zeros((EMBEDDING_DIM,))])
# WORD2VEC_EMBEDDING = gensim.downloader.load('glove-wiki-gigaword-300')
WORD2VEC_EMBEDDING = glove
# Stopwords = Words that do not add meaning to the sentence
STOPWORDS = stopwords.words("english")

def get_train_data(train_file, max_ratings=MAX_RATINGS):
    # Split training reviews and ratings from the train file
    train_data = []
    train_ratings = []
    with open(train_file) as csvfile:
        reader = csv.DictReader(csvfile, delimiter=',')
        for row in reader:
            train_data.append(row['reviews'])
            ratings = int(row['ratings'])
            # Convert output ratings to one-hot encoding 
            train_ratings.append(list(np.eye(max_ratings)[ratings-1]))
    return train_data, train_ratings

def get_test_data(test_file):
    # Return test reviews given test file
    test_data = []
    with open(test_file) as csvfile:
        reader = csv.DictReader(csvfile, delimiter=',')
        for row in reader:
            test_data.append(row['reviews'])
    return test_data

def get_test_ratings(test_file, max_ratings=MAX_RATINGS):
    # Return test ratings given test file
    test_ratings = []
    with open(test_file) as csvfile:
        reader = csv.DictReader(csvfile, delimiter=',')
        for row in reader:
            ratings = int(row['ratings'])
            # Convert output ratings to one-hot encoding 
            test_ratings.append(list(np.eye(max_ratings)[ratings-1]))
    return test_ratings

def convert_to_lower(text):
    # return the reviews after convering then to lowercase
    return text.lower()

def remove_punctuation(text):
    # return the reviews after removing punctuations
    punctuations = ['.', ',', ';', '"', "'", '?', '!', '-', '~', ':', '(', ')', '{', '}', '[', ']', '%', '_', '$', '#', '&', '^', '@']
    for e in punctuations:
        text = text.replace(e, ' ')
    return re.sub('[0-9\s]+', ' ', text)

def remove_stopwords(text):
    # return the reviews after removing the stopwords
    return ' '.join([word for word in text.split() if word not in STOPWORDS])

def perform_tokenization(text):
    # return the reviews after performing tokenization
    return text.split()

def encode_data(text):
    # This function will be used to encode the reviews using a dictionary (created using corpus vocabulary) 
    # return encoded examples
    global WORD_TO_INDEX, EMBEDDING_MATRIX
    vocab_size = len(WORD_TO_INDEX)
    encoding = {}
    for word in text:
        if word not in WORD_TO_INDEX:
            # Add the word to the dictionary
            WORD_TO_INDEX[word] = vocab_size
            # Add the embeddings for the word
            try:
                EMBEDDING_MATRIX = np.append(EMBEDDING_MATRIX, np.reshape(WORD2VEC_EMBEDDING[word], (1, -1)), axis=0)
            except KeyError:
                EMBEDDING_MATRIX = np.append(EMBEDDING_MATRIX, [EMBEDDING_MATRIX[0]], axis=0)
            vocab_size += 1
        encoding[word] = WORD_TO_INDEX[word]
    return encoding

def perform_padding(data, max_input_length=MAX_INPUT_LENGTH):
    # return the reviews after padding the reviews to maximum length
    # input data would be a dict of encoded review
    return [val for val in list(data.values())[:max_input_length]] + [0] * (max_input_length - len(data))

def preprocess_data(data, max_input_length=MAX_INPUT_LENGTH):
    # make all the following function calls on your data
    # return processed data
    # Data is a list of reviews
    # preprocessed_data = []
    processed_data = []
    # max_length = 0
    for review in data:
        review = convert_to_lower(review)
        review = remove_punctuation(review)
        review = remove_stopwords(review)
        review = perform_tokenization(review)
        # max_length = max(len(review), max_length)
        review = encode_data(review)
        review = perform_padding(review, max_input_length)
        processed_data.append(review)
    # for review in preprocessed_data:
    #     review = perform_padding(review, max_input_length)
    #     processed_data.append(review)
    # print(max_length)
    print("max_len = ",max([len(x) for x in processed_data]))
    return processed_data

# @tf.keras.utils.register_keras_serializable()
class TransformerBlock(layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, name=None, rate=0.1,**kwargs):
        super(TransformerBlock, self).__init__(**kwargs)
        self.embed_dim=embed_dim
        self.num_heads=num_heads
        self.ff_dim=ff_dim
        self.rate=rate
        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = keras.Sequential(
            [layers.Dense(ff_dim, activation="relu"), layers.Dense(embed_dim),]
        )
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = layers.Dropout(rate)
        self.dropout2 = layers.Dropout(rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)
    
    def get_config(self):
        config = super().get_config().copy()
        config.update({
            'embed_dim': self.embed_dim,
            'num_heads': self.num_heads,
            'ff_dim': self.ff_dim,
            'rate': self.rate,
            'attn': self.att,
            'ffn': self.ffn,
            'layernorm1': self.layernorm1,
            'layernorm2': self.layernorm2,
            'dropout1': self.dropout1,
            'dropout2': self.dropout2,
        })
        return config
    
    @classmethod
    def from_config(cls, config):
        return cls(**config)

# @tf.keras.utils.register_keras_serializable()
class TokenAndPositionEmbedding(layers.Layer):
    def __init__(self, maxlen, vocab_size, embed_dim,emb_matrix, name=None,**kwargs):
        super(TokenAndPositionEmbedding, self).__init__(**kwargs)
        self.embed_dim=embed_dim
        self.maxlen=maxlen
        self.vocab_size=vocab_size
        self.emb_matrix=emb_matrix
        # self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)
        self.token_emb = layers.Embedding(vocab_size, embed_dim,embeddings_initializer=Constant(emb_matrix), trainable=False)
        # Embedding(self.vocab_size, self.embedding_dim, embeddings_initializer=Constant(self.embedding_matrix), trainable=False, name='embedding')
        self.pos_emb = layers.Embedding(maxlen, embed_dim,embeddings_initializer=Constant(emb_matrix), trainable=False)

    def call(self, x):
        maxlen = tf.shape(x)[-1]
        positions = tf.range(start=0, limit=maxlen, delta=1)
        positions = self.pos_emb(positions)
        x = self.token_emb(x)
        return x + positions
    
    def get_config(self):
        config = super().get_config().copy()
        config.update({
            'embed_dim': self.embed_dim,
            'vocab_size': self.vocab_size,
            'maxlen': self.maxlen,
            'token_emb': self.token_emb,
            'pos_emb': self.pos_emb,
            'emb_matrix': self.emb_matrix,
        })
        return config
    
    @classmethod
    def from_config(cls, config):
      return cls(**config)

def f1_m(y_true, y_pred):
    precision = Precision(y_true, y_pred)
    recall = Recall(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

def softmax_activation(x):
    # write your own implementation from scratch and return softmax values (using predefined softmax is prohibited)
    x_max = tf.math.reduce_max(x, axis=1, keepdims=True)
    x_exp = tf.math.exp(tf.math.subtract(x, x_max))
    x_exp_sum = tf.math.reduce_sum(x_exp, axis=1, keepdims=True)
    return tf.divide(x_exp, x_exp_sum)

class NeuralNet:
    # Class for defining the neural network architecture
    def __init__(self, reviews, ratings, vocab_size, embedding_dim, embedding_matrix, max_input_length=30, max_ratings=5, split_size=0.1, epochs=4, batch_size=32):
        # Maximum length of input sequences
        self.max_input_length = max_input_length
        # Maximum ratings for a review
        self.max_ratings = max_ratings
        # Hyperparameters
        self.epochs = epochs
        self.batch_size = batch_size
        # Length of the vocabulary
        self.vocab_size = vocab_size
        # Embedding matrix
        self.embedding_dim = embedding_dim
        self.embedding_matrix = embedding_matrix
        # Split dataset into training and validation sets
        self.reviews_train, self.reviews_validation, self.ratings_train, self.ratings_validation = train_test_split(reviews, ratings, test_size=split_size)
        # Print insights about the data
        self.print_insights(ratings)

    def build_nn(self):
        #add the input and output layer here; you can use either tensorflow or pytorch
        # model = Sequential()
        input=layers.Input(shape=(self.max_input_length, ))
        token_emb = layers.Embedding(self.vocab_size, self.embedding_dim, embeddings_initializer=Constant(self.embedding_matrix), input_length=self.max_input_length, trainable=False)
        positions = tf.range(start=0, limit=self.max_input_length, delta=1)
        x=token_emb(input)
        y=token_emb(positions)
        combined = layers.Add()([x, y])
        att = layers.MultiHeadAttention(num_heads=2, key_dim=32)
        x=att(x,x)
        x=layers.Dropout(0.1)(x)
        x=layers.LayerNormalization(epsilon=1e-6)(x)
        x=layers.Dense(30, activation="relu")(x)
        x=layers.Dense(self.embedding_dim)(x)
        x=layers.Dropout(0.1)(x)
        x=layers.LayerNormalization(epsilon=1e-6)(x)
        # att = layers.MultiHeadAttention(num_heads=2, key_dim=32)
        # x=att(x,x)
        # x=layers.Dropout(0.1)(x)
        # x=layers.LayerNormalization(epsilon=1e-6)(x)
        # x=layers.Dense(64, activation="relu")(x)
        # x=layers.Dense(self.embedding_dim)(x)
        # x=layers.Dropout(0.1)(x)
        # x=layers.LayerNormalization(epsilon=1e-6)(x)
        # TokenAndPositionEmbedding(self.max_input_length, self.vocab_size, self.embedding_dim, self.embedding_matrix, name='TokenAndPositionEmbedding'))
        # model.add(TransformerBlock(self.embedding_dim, 2, 32, name='TransformerBlock'))
        # # model.add(Embedding(self.vocab_size, self.embedding_dim, embeddings_initializer=Constant(self.embedding_matrix), trainable=False, name='embedding'))
        x=layers.GlobalAveragePooling1D()(x)
        # model.add(Dropout(0.1))
        # x=Flatten()(x)
        x=layers.Dropout(0.2)(x)
        x=Dense(500)(x)
        x=BatchNormalization()(x)
        x=Activation("relu")(x)
        x=layers.Dropout(0.2)(x)
        # x=Dense(20)(x)
        # x=Activation("relu")(x)
        # x=layers.Dropout(0.2)(x)
        x=Dense(20, activation="relu")(x)
        
        x=Dense(MAX_RATINGS)(x)
        x=Activation(softmax_activation, name='softmax')(x)
        # model.add(Dense(MAX_RATINGS, name='dense_1'))
        # model.add(Activation(softmax_activation, name='softmax'))
        self.model = Model(inputs=input, outputs=x)
        print(self.model.summary())

    def train_nn(self):
        # from sklearn.metrics import f1_score
        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath='/content/drive/MyDrive/NLP4/models/5.h5',save_weights_only=False,monitor='val_accuracy',\
                                                                       mode='max',save_best_only=False)
        es = tf.keras.callbacks.EarlyStopping(monitor="val_loss",min_delta=.001,patience=5,verbose=0,restore_best_weights=False)
        # EarlyStopping(monitor='val_loss', mode='min', verbose=1)
        self.model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy', Precision(), Recall()])
        print(tf.keras.utils.plot_model(self.model, show_shapes=True, to_file='Model1.png'))
        from IPython.display import Image
        Image(retina=True, filename='Model1.png')
        
        self.model.fit(x=self.reviews_train, y=self.ratings_train, batch_size=self.batch_size, epochs=self.epochs, verbose=1, validation_data=(self.reviews_validation, self.ratings_validation))
        self.model.evaluate(x=self.reviews_validation, y=self.ratings_validation)
        # print(self.model.get_config())
        self.model.save(f'/content/drive/MyDrive/NLP4/models/5.h5')
    
    def predict(self, reviews):
        # return a list containing all the ratings predicted by the trained model
        return self.model.predict(reviews)

    def load_nn():
        # function to load the neural network with relu activation used
        return load_model(f'models/5.h5', custom_objects={'softmax_activation': softmax_activation})#, 'TransformerBlock': TransformerBlock,'TokenAndPositionEmbedding': TokenAndPositionEmbedding})

    def print_insights(self, ratings):
        # Print information about the training data available
        ratings = [np.where(r==1)[0][0] for r in np.array(ratings)]
        ratings, count = np.unique(ratings, return_counts=True)
        for r, c in zip(ratings, count):
            print(f'Ratings: {r+1} => {c}') 

    def write_file(self, filename='output/validation_output.txt'):
        # Function to write predicted values on the validation set to file
        predictions = self.predict(self.reviews_validation)
        with open(filename, 'w') as file:
            for prediction, actual in zip(predictions, self.ratings_validation):
                file.write(f'{prediction.argmax() + 1}, {actual.argmax() + 1}\n')

def main(train_file, test_file, load_model=False):
    # get the test data and preprocess it
    test_data = get_test_data(test_file)
    test_reviews = preprocess_data(test_data)
    test_ratings = get_test_ratings(test_file)
    if load_model:
        # load the pretrained model
        model = NeuralNet.load_nn()
        return model.predict(test_reviews)
    else:
        # Hyperparameters
        batch_size, epochs = 256, 30
        # Read data from the training file and split the input and output
        train_data, train_ratings = get_train_data(train_file)
        # Preprocess the training reviews
        train_reviews = preprocess_data(train_data)
        # build the model and train it
        
        nn = NeuralNet(train_reviews, train_ratings, len(WORD_TO_INDEX), EMBEDDING_DIM, EMBEDDING_MATRIX, epochs=epochs, batch_size=batch_size)
        nn.build_nn()
        nn.train_nn()

# Main function to run code from command line
# if __name__ == '__main__':
    
#     main('/content/drive/MyDrive/NLP3/train_balanced.csv', '/content/drive/MyDrive/NLP3/gold_test.csv', load_model=False)
    # predictions = predictions_softmax.argmax(axis=1) + 1

    # Write the predictions to a file
    # with open('/predict.txt', 'w') as file:
    #     for prediction in predictions:
    #         file.write(f'{prediction}\n')

# test_data = get_test_data('/content/drive/MyDrive/NLP3/gold_test.csv')
# true_ratings = get_test_ratings('/content/drive/MyDrive/NLP3/gold_test.csv')
# test_ratings = [r.index(1.0)+1 for r in list(true_ratings)]

def predict(sentence):
    nn = NeuralNet.load_nn()
    reviews_processed = preprocess_data([sentence]) # preprocess the data
    predictions_softmax = nn.predict(reviews_processed) # get the predictions from the model
    return predictions_softmax

# test_size = len(predictions)
# match_count = 0

# for i in range(test_size):
#     if predictions[i] == test_ratings[i]:
#         match_count += 1

# acc = match_count*100.0/test_size

# print('> test accuracy = %.3f' % (acc))
# print('\n')

# y_true = test_ratings
# y_pred = predictions

# print(sklearn.metrics.classification_report(y_true, y_pred))
# print(sklearn.metrics.confusion_matrix(y_true, y_pred))

# x=abs(y_pred-y_true)
# idx=np.argwhere(x==2).reshape(-1)
# idx

# for i in idx[:15]:
#   print(test_data[i])
#   print("true_label  ",y_true[i])
#   print("pred_label  ",y_pred[i])



# len(reviews_processed)

# train_data, train_ratings = get_train_data('/content/drive/MyDrive/NLP3/train_balanced.csv')
# train_reviews = np.array(preprocess_data(train_data))
# train_reviews.shape

# label=np.sum(train_ratings*np.array([1,2,3,4,5]),axis=1)
# label.shape

# x1=np.argwhere(label==1).reshape(-1)
# print(len(x1))
# x2=np.argwhere(label==2).reshape(-1)
# print(len(x2))
# x3=np.argwhere(label==3).reshape(-1)
# print(len(x3))
# x4=np.argwhere(label==4).reshape(-1)
# print(len(x4))
# x5=np.argwhere(label==5).reshape(-1)
# print(len(x5))
# # x2=np.argwhere(train_ratings==np.array([0.0,1.0,0.0,0.0,0.0]))[:,0]
# # print(x2.shape)
# # x3=np.argwhere(train_ratings==np.array([0.0,0.0,1.0,0.0,0.0]))[:,0]
# # print(x3.shape)
# # x4=np.argwhere(train_ratings==np.array([0.0,0.0,0.0,1.0,0.0]))[:,0]
# # print(x4.shape)
# # x5=np.argwhere(train_ratings==np.array([0.0,0.0,0.0,0.0,1.0]))[:,0]
# # print(x5.shape)
# x1

# a=train_reviews[x1].reshape(-1)
# b=train_reviews[x2].reshape(-1)
# c=train_reviews[x3].reshape(-1)
# d=train_reviews[x4].reshape(-1)
# e=train_reviews[x5].reshape(-1)
# print(len(np.unique(a)),len(np.unique(b)),len(np.unique(c)),len(np.unique(d)),len(np.unique(e)))
# print(len(np.intersect1d(a, b)))
# print(len(np.intersect1d(b, c)))
# print(len(np.intersect1d(c, d)))
# print(len(np.intersect1d(d, e)))
# # print(len(np.intersect1d(a, b)))

# train_reviews[x1]

